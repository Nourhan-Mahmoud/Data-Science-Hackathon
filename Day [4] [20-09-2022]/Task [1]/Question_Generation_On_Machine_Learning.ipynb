{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-auWDi1dnAN"
   },
   "source": [
    "# Wikipedia-API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cBpmJ23SR0X"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# install the latest release of Haystack\n",
    "pip install --upgrade pip\n",
    "pip install git+https://github.com/deepset-ai/haystack.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZIMsesECxEMo"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
    "logging.getLogger(\"haystack\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2rxbgu_Xirl"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from haystack.nodes import QuestionGenerator, BM25Retriever, FARMReader\n",
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "from haystack.pipelines import (\n",
    "    QuestionGenerationPipeline,\n",
    "    RetrieverQuestionGenerationPipeline,\n",
    "    QuestionAnswerGenerationPipeline,\n",
    ")\n",
    "from haystack.utils import launch_es, print_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-O-1MGai_M6"
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers==3.0.0\n",
    "!python -m nltk.downloader punkt\n",
    "!git clone https://github.com/patil-suraj/question_generation.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jW0HLuh9t2A2"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/boudinfl/pke.git\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wB6__BYMFqBX"
   },
   "outputs": [],
   "source": [
    "!pip install flashtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8rt9bu2GrJ1"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/amontgomerie/question_generator/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27_EOm3fdGks"
   },
   "outputs": [],
   "source": [
    "!pip install Wikipedia-API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sc-HtJvpd0fL"
   },
   "outputs": [],
   "source": [
    "import wikipediaapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHb35V6ud27i"
   },
   "outputs": [],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "\n",
    "ml_art = wiki_wiki.page('Machine_Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqESymwJd_aC"
   },
   "outputs": [],
   "source": [
    "print(\"Page - Exists: %s\" % ml_art.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8ipIWzQeGkk"
   },
   "outputs": [],
   "source": [
    "print(\"Page - Title: %s\" % ml_art.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38nBj--MeOg9"
   },
   "outputs": [],
   "source": [
    "print(\"Page - Summary: %s\" % ml_art.summary[0:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DfkcvDxheRvO"
   },
   "outputs": [],
   "source": [
    "print(ml_art.fullurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YemVEcWyfYl4"
   },
   "outputs": [],
   "source": [
    "ml_ftxt = ml_art.text\n",
    "ml_ftxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IhP4fu3pee9K"
   },
   "outputs": [],
   "source": [
    "ml_summary = ml_art.summary\n",
    "ml_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9x9PN4FUfm96"
   },
   "source": [
    "# Clean ML Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lt8kYm81grci"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcXPujDzgvOp"
   },
   "outputs": [],
   "source": [
    "ml_ftxt = ml_ftxt.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bKuRPP-g7UN"
   },
   "outputs": [],
   "source": [
    "ml_ftxt = re.sub(r'\\n', '', ml_ftxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqgR6lmVhGLx"
   },
   "outputs": [],
   "source": [
    "ml_ftxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2vhdWRen22E"
   },
   "outputs": [],
   "source": [
    "ml_ftxt = re.sub(r'\\-', '', ml_ftxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXFqDA-Yn2oi"
   },
   "outputs": [],
   "source": [
    "ml_ftxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rd6eqXDaoT96"
   },
   "outputs": [],
   "source": [
    "ml_ftxt = re.sub(r'\\u200a','', ml_ftxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XU9YhYJKoT6J"
   },
   "outputs": [],
   "source": [
    "\n",
    "ml_ftxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjUcY3mKhH8i"
   },
   "outputs": [],
   "source": [
    "# Remove puncuation\n",
    "my_punctuation = string.punctuation.replace(\",\", \"\")\n",
    "my_punctuation = my_punctuation.replace(\".\", \"\")\n",
    "\n",
    "translator = str.maketrans('', '', my_punctuation)\n",
    "ml_ftxt = ml_ftxt.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhhnQQ_ZhZKl"
   },
   "outputs": [],
   "source": [
    "\n",
    "ml_ftxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CT0WtJcjhan8"
   },
   "outputs": [],
   "source": [
    "# Remove numbers\n",
    "ml_ftxt = re.sub(r'[0-9]', '', ml_ftxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOyWFukxnkT8"
   },
   "outputs": [],
   "source": [
    "ml_ftxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QVhM10JMpcdf"
   },
   "outputs": [],
   "source": [
    "ml_ftxt = re.sub(r' - ','', ml_ftxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNBZ_AG7pcz1"
   },
   "outputs": [],
   "source": [
    "ml_ftxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oE3_cthUnrjf"
   },
   "source": [
    "# T5 With text Data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkYeu1-enb4J"
   },
   "outputs": [],
   "source": [
    "# Max length 265.\n",
    "txt1 = \"machine learning ml is a field of inquiry devoted to understanding and building methods that learn, that is, methods that leverage data to improve performance on some set of tasks. it is seen as a part of artificial intelligence.\"\n",
    "txt2 = \"the study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning. some implementations of machine learning use data and neural networks in a way that mimics the working of a biological brain.  in its application across business problems, machine learning is also referred to as predictive analytics.overviewlearning algorithms work on the basis that strategies, algorithms, and inferences that worked well in the past are likely to continue working well in the future. these inferences can be obvious, such as since the sun rose every morning for the last , days, it will probably rise tomorrow morning as well. they can be nuanced, such as x of families have geographically separate species with color variants, so there is a y chance that undiscovered black swans exist.\"\n",
    "txt3 = \"machine learning programs can perform tasks without being explicitly programmed to do so. it involves computers learning from data provided so that they carry out certain tasks. for simple tasks assigned to computers, it is possible to program algorithms telling the machine how to execute all steps required to solve the problem at hand on the computers part, no learning is needed. for more advanced tasks, it can be challenging for a human to manually create the needed algorithms. in practice, it can turn out to be more effective to help the machine develop its own algorithm, rather than having human programmers specify every needed step.the discipline of machine learning employs various approaches to teach computers to accomplish tasks where no fully satisfactory algorithm is available. in cases where vast numbers of potential answers exist, one approach is to label some of the correct answers as valid. this can then be used as training data for the computer to improve the algorithms it uses to determine correct answers. for example, to train a system for the task of digital character recognition, the mnist dataset of handwritten digits has often been used.history and relationships to other fieldsthe term machine learning was coined in  by arthur samuel, an ibm employee and pioneer in the field of computer gaming and artificial intelligence. also the synonym selfteaching computers were used in this time period.by the early s an experimental learning machine with punched tape memory, called cybertron, had been developed by raytheon company to analyze sonar signals, electrocardiograms and speech patterns using rudimentary reinforcement learning. it was repetitively trained by a human operatorteacher to recognize patterns and equipped with a goof button to cause it to reevaluate incorrect decisions.\"\n",
    "txt4 = \"a representative book on research into machine learning during the s was nilssons book on learning machines, dealing mostly with machine learning for pattern classification. interest related to pattern recognition continued into the s, as described by duda and hart in . in  a report was given on using teaching strategies so that a neural network learns to recognize  characters  letters,  digits, and  special symbols from a computer terminal.tom m. mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field a computer program is said to learn from experience e with respect to some class of tasks t and performance measure p if its performance at tasks in t, as measured by p,  improves with experience e. this definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. this follows alan turings proposal in his paper computing machinery and intelligence, in which the question can machines think is replaced with the question can machines do what we as thinking entities can do.\"\n",
    "txt5 = \"modern day machine learning has two objectives, one is to classify data based on models which have been developed, the other purpose is to make predictions for future outcomes based on these models.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ByTXYKVlqxUC"
   },
   "outputs": [],
   "source": [
    "%cd question_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVgBO2t4q1Vz"
   },
   "outputs": [],
   "source": [
    "from pipelines import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5uwydhTq4E0"
   },
   "outputs": [],
   "source": [
    "model = pipeline(\"question-generation\", model=\"valhalla/t5-base-qg-hl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r3TNerq8jeZR"
   },
   "outputs": [],
   "source": [
    "len(txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8CjpiCEq_cH"
   },
   "outputs": [],
   "source": [
    "model(txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mBlNXpfrRQW"
   },
   "outputs": [],
   "source": [
    "model(txt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBYq4r7vrh9F"
   },
   "outputs": [],
   "source": [
    "model(txt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Kz6Wvykrpyo"
   },
   "outputs": [],
   "source": [
    "model(txt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSGFDp4PrtqO"
   },
   "outputs": [],
   "source": [
    "model(txt5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQNFA87ysc4_"
   },
   "source": [
    "# Get Keypharses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuHTuCOXNhOv"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YyNu4bsF9bP4"
   },
   "outputs": [],
   "source": [
    "import pke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJtnu_rutNlJ"
   },
   "outputs": [],
   "source": [
    "All_top = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WT_3iAzvutbH"
   },
   "outputs": [],
   "source": [
    "txt = ml_ftxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D031YGoyrHY8"
   },
   "source": [
    "## Unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yn1jO2QNOwJP"
   },
   "source": [
    "### Graph Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pyu08LFqJKK"
   },
   "source": [
    "#### TopicRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDQ_YDrc_Xa3"
   },
   "source": [
    "**NOTES on TopicRank**:\n",
    "* unsupervised graph-based ranking model to keyphrase extraction\n",
    "* uses a random walk algorithm -> to estimate the importance of each topic (node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbVmxaCPp7Uf"
   },
   "outputs": [],
   "source": [
    "# initialize a TopicRank keyphrase extraction model\n",
    "extractor = pke.unsupervised.TopicRank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-VKU8AGAXYR"
   },
   "outputs": [],
   "source": [
    "extractor.load_document(input=txt, language='en') # used to pre-process the text (sentence splitting, tokenization, Part-of-Speech tagging, stemming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BK2FOJSnoUqW"
   },
   "outputs": [],
   "source": [
    "extractor.grammar_selection(grammar=\"NP: {<ADJ>*<NOUN|PROPN>+}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbBwPbDQC3St"
   },
   "outputs": [],
   "source": [
    "extractor.candidate_selection()  #identifying keyphrase candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkSK3-2EIFB5"
   },
   "source": [
    " In **TopicRank**, candidate weighting is a three-step process:\n",
    "1. candidate clustering (grouping keyphrase candidates into topics)\n",
    "2. graph construction (building a complete-weighted-graph of topics)\n",
    "3. rank topics (nodes) using a random walk algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_c5rs8sRGnGN"
   },
   "outputs": [],
   "source": [
    "extractor.candidate_weighting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPEG9l_XL76f"
   },
   "outputs": [],
   "source": [
    "# Get the N-best candidates (here, 5) as keyphrases\n",
    "keyphrases = extractor.get_n_best(n=20, stemming=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QB6UrF7fouOS"
   },
   "outputs": [],
   "source": [
    "All_top.extend(keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mrtOC6FuHxk"
   },
   "outputs": [],
   "source": [
    "All_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMVK_XHbqbK4"
   },
   "source": [
    "#### MultipartiteRank Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yChGgSAEqiiV"
   },
   "outputs": [],
   "source": [
    "extractor = pke.unsupervised.MultipartiteRank()\n",
    "\n",
    "extractor.load_document(input=txt, language='en')\n",
    "\n",
    "extractor.grammar_selection(grammar=\"NP: {<ADJ>*<NOUN|PROPN>+}\")\n",
    "\n",
    "extractor.grammar_selection()\n",
    "\n",
    "extractor.candidate_weighting()\n",
    "\n",
    "keyphrases = extractor.get_n_best(n=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ktwwug1q2Zi"
   },
   "outputs": [],
   "source": [
    "All_top.extend(keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQmloPIhuTy5"
   },
   "outputs": [],
   "source": [
    "All_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtYnapIMtZMY"
   },
   "source": [
    "#### TopicalPageRank Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ydokqm3htesM"
   },
   "outputs": [],
   "source": [
    "extractor = pke.unsupervised.TopicalPageRank()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=txt,\n",
    "                        language='en')\n",
    "\n",
    "extractor.grammar_selection(grammar=\"NP: {<ADJ>*<NOUN|PROPN>+}\")\n",
    "\n",
    "# 3. select the noun phrases as keyphrase candidates.\n",
    "extractor.candidate_selection()\n",
    "\n",
    "# 4. weight the keyphrase candidates using Single Topical PageRank.\n",
    "#    Builds a word-graph in which edges connecting two words occurring\n",
    "#    in a window are weighted by co-occurrence counts.\n",
    "extractor.candidate_weighting()\n",
    "\n",
    "# 5. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases = extractor.get_n_best(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0J8H0F5txfQ"
   },
   "outputs": [],
   "source": [
    "All_top.extend(keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVdMShnourYg"
   },
   "outputs": [],
   "source": [
    "All_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBKBjOjzNjm8"
   },
   "source": [
    "### Statistical models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B596beOzNrQH"
   },
   "source": [
    "#### FirstPhrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-veVuRegTBz1"
   },
   "outputs": [],
   "source": [
    " # 1. create a FirstPhrases baseline extractor.\n",
    "extractor = pke.unsupervised.FirstPhrases()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=txt,language='en')\n",
    "\n",
    "extractor.grammar_selection(grammar=\"NP: {<ADJ>*<NOUN|PROPN>+}\")\n",
    "\n",
    "# 3. select the longest sequences of nouns and adjectives as candidates.\n",
    "extractor.candidate_selection()\n",
    "\n",
    "# 4. weight the candidates using their position\n",
    "extractor.candidate_weighting()\n",
    "\n",
    "# 5. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases = extractor.get_n_best(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzYdusjGjV9v"
   },
   "outputs": [],
   "source": [
    "All_top.extend(keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UzuR4CpLu2EY"
   },
   "outputs": [],
   "source": [
    "All_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEzuRj_PScOH"
   },
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnKoh6b3Sg8N"
   },
   "outputs": [],
   "source": [
    "extractor = pke.unsupervised.TfIdf()        # initialize a keyphrase extraction model, here TFxIDF\n",
    "\n",
    "extractor.load_document(input=txt)       # load the content of the document (str or spacy Doc)\n",
    "\n",
    "extractor.grammar_selection(grammar=\"NP: {<ADJ>*<NOUN|PROPN>+}\")\n",
    "\n",
    "extractor.candidate_selection()             # identify keyphrase candidates\n",
    "\n",
    "extractor.candidate_weighting()             # weight keyphrase candidates\n",
    "\n",
    "keyphrases = extractor.get_n_best(n=20)      # select the 5-best candidates as keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ejxac1NqSmlj"
   },
   "outputs": [],
   "source": [
    "All_top.extend(keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRECIEfTu_vj"
   },
   "outputs": [],
   "source": [
    "All_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtoKjz3uk8TE"
   },
   "source": [
    "#### KPMiner Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XU6zkC1aSr6M"
   },
   "outputs": [],
   "source": [
    "# 1. create a KPMiner extractor.\n",
    "extractor = pke.unsupervised.KPMiner()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=txt,language='en')\n",
    "\n",
    "# 3. select {1-5}-grams that do not contain punctuation marks or\n",
    "#    stopwords as keyphrase candidates. Set the least allowable seen\n",
    "#    frequency to 5 and the number of words after which candidates are\n",
    "#    filtered out to 200.\n",
    "lasf = 5\n",
    "cutoff = 200\n",
    "extractor.candidate_selection(lasf=lasf, cutoff=cutoff)\n",
    "\n",
    "# 4. weight the candidates using KPMiner weighting function.\n",
    "#df = pke.load_document_frequency_file(input_file=\"path/to/df.tsv.gz\")\n",
    "\n",
    "#alpha = 2.3\n",
    "#sigma = 3.0\n",
    "# df=df, alpha=alpha, sigma=sigma\n",
    "\n",
    "extractor.candidate_weighting()\n",
    "\n",
    "# 5. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases = extractor.get_n_best(n=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCBeWvQ6kxhk"
   },
   "outputs": [],
   "source": [
    "All_top.extend(keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FD-8M6r7mxF2"
   },
   "outputs": [],
   "source": [
    "All_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13c9elE_pD4t"
   },
   "source": [
    "## Supervised\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YQmCNAZpNVw"
   },
   "source": [
    "#### Kea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EyLD42bRpDKT"
   },
   "outputs": [],
   "source": [
    "# 1. create a Kea extractor.\n",
    "extractor = pke.supervised.Kea()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "stoplist = pke.lang.stopwords.get('en')\n",
    "extractor.load_document(input=txt, language='en')\n",
    "\n",
    "# 3. select 1-3 grams that do not start or end with a stopword as\n",
    "#    candidates. Candidates that contain punctuation marks as words\n",
    "#    are discarded.\n",
    "extractor.candidate_selection()\n",
    "\n",
    "# 4. classify candidates as keyphrase or not keyphrase.\n",
    "#df = pke.load_document_frequency_file(input_file='path/to/df.tsv.gz')\n",
    "#model_file = 'path/to/kea_model'\n",
    "#model_file=model_file,df=df\n",
    "extractor.candidate_weighting()\n",
    "\n",
    "# 5. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases = extractor.get_n_best(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsPbeX_rpqaY"
   },
   "outputs": [],
   "source": [
    "All_top.extend(keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ieewaNZPpsNa"
   },
   "outputs": [],
   "source": [
    "All_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PScxChevXz_"
   },
   "source": [
    "# Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68rSGw6Mv0jf"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owkepdkxvQwn"
   },
   "outputs": [],
   "source": [
    "x = np.array(All_top)\n",
    "u = np.unique(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iigbS-ZmwAMu"
   },
   "outputs": [],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5NrfLKbyvhV"
   },
   "outputs": [],
   "source": [
    "z = u[130:]\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZEKw-t0vgnx"
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in z:\n",
    "  cnt = 0\n",
    "  for ii in range(len(All_top)):\n",
    "    if i == All_top[ii][0]:\n",
    "      cnt += 1\n",
    "  if cnt >=2:\n",
    "      l.append(i)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0i3Jp7XVxl2D"
   },
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTuYF7Nd2kJd"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "  \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from flashtext import KeywordProcessor\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    sentences = [sent_tokenize(text)]\n",
    "    sentences = [y for x in sentences for y in x]\n",
    "    # Remove any short sentences less than 20 letters.\n",
    "    sentences = [sentence.strip() for sentence in sentences if len(sentence) > 20]\n",
    "    return sentences\n",
    "\n",
    "def get_sentences_for_keyword(keywords, sentences):\n",
    "    keyword_processor = KeywordProcessor()\n",
    "    keyword_sentences = {}\n",
    "    for word in keywords:\n",
    "        keyword_sentences[word] = []\n",
    "        keyword_processor.add_keyword(word)\n",
    "    for sentence in sentences:\n",
    "        keywords_found = keyword_processor.extract_keywords(sentence)\n",
    "        for key in keywords_found:\n",
    "            keyword_sentences[key].append(sentence)\n",
    "\n",
    "    for key in keyword_sentences.keys():\n",
    "        values = keyword_sentences[key]\n",
    "        values = sorted(values, key=len, reverse=True)\n",
    "        keyword_sentences[key] = values\n",
    "    return keyword_sentences\n",
    "\n",
    "sentences = tokenize_sentences(txt)\n",
    "keyword_sentence_mapping = get_sentences_for_keyword(l, sentences)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnswTOgh5KSl"
   },
   "outputs": [],
   "source": [
    "keyword_sentence_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EAvStMTCM-3"
   },
   "source": [
    "# T5 Model with Keywords sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhYwX9YLw67_"
   },
   "outputs": [],
   "source": [
    "with_words = pipeline(\"question-generation\", model=\"valhalla/t5-base-qg-hl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dh5cqthjykAV"
   },
   "outputs": [],
   "source": [
    "type(keyword_sentence_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZiwMCyFK9FpL"
   },
   "outputs": [],
   "source": [
    "li = list(keyword_sentence_mapping.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y2GqQ9-I-LKz"
   },
   "outputs": [],
   "source": [
    "ke = list(keyword_sentence_mapping.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6huqMQu9Hn9"
   },
   "outputs": [],
   "source": [
    "len(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "haLRIWCG-Ptm"
   },
   "outputs": [],
   "source": [
    "len(ke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fMMnHpwJA64"
   },
   "outputs": [],
   "source": [
    "ke[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-kmpxcA9ouN"
   },
   "outputs": [],
   "source": [
    "li[0][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUSlwbgh_T7A"
   },
   "outputs": [],
   "source": [
    "xx = []\n",
    "for i in range(len(li[0])):\n",
    "  if len(li[0][i]) < 256:\n",
    "    print(i)\n",
    "    xx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NrzPGCheE6yw"
   },
   "outputs": [],
   "source": [
    "li[0][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTxgdvvPEBMm"
   },
   "outputs": [],
   "source": [
    "with_words(li[0][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glq1nyG_Ah4J"
   },
   "outputs": [],
   "source": [
    "for z in range(len(xx)):\n",
    "  print(xx[z])\n",
    "  with_words(li[0][xx[z]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQdCozBQ9cqH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QsHzgW-EGvH"
   },
   "source": [
    "# T5 Question Generation with format like in hugging face\n",
    "\n",
    "`<answer>Answer<context>Context`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nn7Q0Np2GS9_"
   },
   "outputs": [],
   "source": [
    "li = list(keyword_sentence_mapping.values())\n",
    "ke = list(keyword_sentence_mapping.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBX9_BCaGXFu"
   },
   "outputs": [],
   "source": [
    "ar = \"<answer>\"+ke[0][8]+\"<context>\"+li[0][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-gU2J1TIsyY"
   },
   "outputs": [],
   "source": [
    "li[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGKVl0T-Gvi2"
   },
   "outputs": [],
   "source": [
    "%cd question_generator/\n",
    "%load questiongenerator.py\n",
    "from questiongenerator import QuestionGenerator\n",
    "from questiongenerator import print_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Um02iAr7G7k0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "assert device == torch.device('cuda'), \"Not using CUDA. Set: Runtime > Change runtime type > Hardware Accelerator: GPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DdwJumhDHe4q"
   },
   "outputs": [],
   "source": [
    "qg = QuestionGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kudc8ZopG8wf"
   },
   "outputs": [],
   "source": [
    "qa_list = qg.generate(\n",
    "    ar, \n",
    "    num_questions=10, \n",
    "    answer_style='all'\n",
    ")\n",
    "print_qa(qa_list)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "C-auWDi1dnAN",
    "9x9PN4FUfm96",
    "oE3_cthUnrjf",
    "TQNFA87ysc4_",
    "D031YGoyrHY8",
    "yn1jO2QNOwJP",
    "9pyu08LFqJKK",
    "bMVK_XHbqbK4",
    "MtYnapIMtZMY",
    "wBKBjOjzNjm8",
    "B596beOzNrQH",
    "qEzuRj_PScOH",
    "OtoKjz3uk8TE",
    "_YQmCNAZpNVw",
    "5PScxChevXz_"
   ],
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
